{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b735cf1-770d-452b-94cb-2a37ad9b16f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Model Comparison & Feature Engineering**\n",
    "\n",
    "This notebook demonstrates a full machine learning workflow for predicting product price using Databricks. \n",
    "\n",
    "Train multiple machine learning models, perform hyperparameter tuning, analyze feature importance, and build Spark ML pipelines. The tasks include training three different models, comparing their metrics using MLflow, constructing a Spark ML pipeline, and selecting the best-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80b3d37-bca4-4289-b273-f9d7665935e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task-1 Train 3 different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "866ce78f-ad1b-4907-a0a5-3f50bee2a20f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load spark table\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "events = spark.table(\"workspace.default.silver_ecommerce_events_event_type_part\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8265a0a1-0648-4e94-a77f-8de497ae42f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert data types\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "events = events.withColumn(\"product_id\", F.col(\"product_id\").cast(\"long\")) \\\n",
    "               .withColumn(\"user_id\", F.col(\"user_id\").cast(\"long\")) \\\n",
    "               .withColumn(\"price\", F.col(\"price\").cast(\"double\")) \\\n",
    "               .withColumn(\"category_id\", F.col(\"category_id\").cast(\"long\")) \\\n",
    "               .withColumn(\"event_time\", F.col(\"event_time\").cast(\"timestamp\")) \\\n",
    "               .withColumn(\"event_date\", F.to_date(\"event_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f749b7-d1a1-4ac2-83e0-7a572af2450d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>user_id</th><th>price</th></tr></thead><tbody><tr><td>1004858</td><td>522850155</td><td>131.53</td></tr><tr><td>1004864</td><td>515027636</td><td>115.81</td></tr><tr><td>1004767</td><td>550404843</td><td>245.28</td></tr><tr><td>1004768</td><td>557311182</td><td>245.26</td></tr><tr><td>1004838</td><td>564297430</td><td>141.29</td></tr><tr><td>1004992</td><td>512430659</td><td>231.64</td></tr><tr><td>1004766</td><td>563561200</td><td>244.32</td></tr><tr><td>1004741</td><td>540014734</td><td>189.97</td></tr><tr><td>1004870</td><td>547109676</td><td>281.11</td></tr><tr><td>1004836</td><td>533886616</td><td>223.76</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1004858,
         522850155,
         131.53
        ],
        [
         1004864,
         515027636,
         115.81
        ],
        [
         1004767,
         550404843,
         245.28
        ],
        [
         1004768,
         557311182,
         245.26
        ],
        [
         1004838,
         564297430,
         141.29
        ],
        [
         1004992,
         512430659,
         231.64
        ],
        [
         1004766,
         563561200,
         244.32
        ],
        [
         1004741,
         540014734,
         189.97
        ],
        [
         1004870,
         547109676,
         281.11
        ],
        [
         1004836,
         533886616,
         223.76
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#select reuquired columns and convert spark table to pandas dataframe to build models\n",
    "selected_df = events.select(\"product_id\", \"user_id\", \"price\")\n",
    "display(selected_df.limit(10))\n",
    "\n",
    "pandas_df = selected_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eea7839b-c05d-416f-a6e4-3f8b71e300cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#select required columns for features and label to build models, feature scale the columns, split data into train and test sets and train a regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = pandas_df[[\"product_id\" , \"user_id\"]]\n",
    "y = pandas_df[\"price\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de64745b-ab8e-4407-9c60-7256ab197a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "models = {\n",
    "    \"linear_regression\": LinearRegression(),\n",
    "    \"decision_tree\": DecisionTreeRegressor(max_depth=5, random_state=42),\n",
    "    #\"random_forest\": RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8e6bc9-de0e-4081-aaa4-3758c00ccf40",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 8"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_regression R2 score: 0.0544\ndecision_tree R2 score: 0.4741\n"
     ]
    }
   ],
   "source": [
    "# Train Models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    r2 = model.score(X_test, y_test)\n",
    "    results[name] = r2\n",
    "    print(f\"{name} R2 score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3c603e-4da8-4767-997d-7156ed55969f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 9"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'linear_regression' already exists. Creating a new version of this model...\nCreated version '3' of model 'workspace.default.linear_regression'.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_regression logged with R2: 0.0544\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'decision_tree' already exists. Creating a new version of this model...\nCreated version '3' of model 'workspace.default.decision_tree'.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree logged with R2: 0.4741\n"
     ]
    }
   ],
   "source": [
    "# Use MLflow experiment\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "for name, model in models.items():\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        mlflow.log_param(\"model_type\", name)\n",
    "        mlflow.log_param(\"features\", \"vdom, card_adds\")\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        r2 = model.score(X_test, y_test)\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=name,\n",
    "            signature=mlflow.models.signature.infer_signature(X_train, model.predict(X_train))\n",
    "        )\n",
    "\n",
    "        mlflow.log_metric(\"r2_score\", r2)\n",
    "        mlflow.set_tag(\"model_type\", name)\n",
    "        mlflow.set_tag(\"input_sample\", X_train[5])\n",
    "\n",
    "        print(f\"{name} logged with R2: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a58072e5-333e-4e2e-9228-39cfd20b4d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task-2 Compare metrics in MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402dc8a7-3d90-40cb-b48e-872c45014c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Visualize the experiment, metrics, parameters, and model in the MLflow tracking UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fdf5825-aef0-4d5b-a150-a876e9d19ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task-3 Build Spark ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581536fd-f843-4f6e-96a4-a4c59d049b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>user_id</th><th>price</th></tr></thead><tbody><tr><td>1004858</td><td>522850155</td><td>131.53</td></tr><tr><td>1004864</td><td>515027636</td><td>115.81</td></tr><tr><td>1004767</td><td>550404843</td><td>245.28</td></tr><tr><td>1004768</td><td>557311182</td><td>245.26</td></tr><tr><td>1004838</td><td>564297430</td><td>141.29</td></tr><tr><td>1004992</td><td>512430659</td><td>231.64</td></tr><tr><td>1004766</td><td>563561200</td><td>244.32</td></tr><tr><td>1004741</td><td>540014734</td><td>189.97</td></tr><tr><td>1004870</td><td>547109676</td><td>281.11</td></tr><tr><td>1004836</td><td>533886616</td><td>223.76</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1004858",
         "522850155",
         "131.53"
        ],
        [
         "1004864",
         "515027636",
         "115.81"
        ],
        [
         "1004767",
         "550404843",
         "245.28"
        ],
        [
         "1004768",
         "557311182",
         "245.26"
        ],
        [
         "1004838",
         "564297430",
         "141.29"
        ],
        [
         "1004992",
         "512430659",
         "231.64"
        ],
        [
         "1004766",
         "563561200",
         "244.32"
        ],
        [
         "1004741",
         "540014734",
         "189.97"
        ],
        [
         "1004870",
         "547109676",
         "281.11"
        ],
        [
         "1004836",
         "533886616",
         "223.76"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load spark table and select required columns\n",
    "spark_df = spark.table(\"workspace.default.silver_ecommerce_events_event_type_part\")\n",
    "display(spark_df.select(\"product_id\", \"user_id\", \"price\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2884ea8-8d38-4e2d-9943-c4a33ba5adb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert datatypes to numeric for required columns\n",
    "spark_df = spark_df.withColumn(\"product_id\", F.col(\"product_id\").cast(\"long\")) \\\n",
    "                   .withColumn(\"user_id\", F.col(\"user_id\").cast(\"long\")) \\\n",
    "                   .withColumn(\"price\", F.col(\"price\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102689e6-811f-40ab-b78e-2c6b7ddd5f3a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 15"
    }
   },
   "outputs": [],
   "source": [
    "# Create feature vector using VectorAssembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"product_id\", \"user_id\"],\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "046464e2-a6a8-4b36-90ca-16c1d1b40610",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 16"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea9cba7-c87e-4982-a122-aa9fd8cba589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build Spark ML Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dbdaad9-bae9-4fd4-b1d5-2516f190ef2d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 18"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>user_id</th><th>price</th><th>prediction</th></tr></thead><tbody><tr><td>1005135</td><td>535871217</td><td>1747.79</td><td>357.3883602446577</td></tr><tr><td>1005073</td><td>543427258</td><td>1207.71</td><td>356.7930357673162</td></tr><tr><td>10900026</td><td>514080443</td><td>40.8</td><td>289.4286870265142</td></tr><tr><td>4802639</td><td>514808401</td><td>218.77</td><td>332.3078321524162</td></tr><tr><td>10800132</td><td>539194858</td><td>22.91</td><td>288.15195724562045</td></tr><tr><td>1005072</td><td>555447922</td><td>1044.09</td><td>355.84526557638407</td></tr><tr><td>1005143</td><td>517062545</td><td>1541.61</td><td>358.87128613985936</td></tr><tr><td>50600000</td><td>514933060</td><td>102.42</td><td>9.80243445393046</td></tr><tr><td>10800025</td><td>539194858</td><td>56.6</td><td>288.152710717548</td></tr><tr><td>1005101</td><td>539538500</td><td>450.46</td><td>357.0994503030662</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1005135,
         535871217,
         1747.79,
         357.3883602446577
        ],
        [
         1005073,
         543427258,
         1207.71,
         356.7930357673162
        ],
        [
         10900026,
         514080443,
         40.8,
         289.4286870265142
        ],
        [
         4802639,
         514808401,
         218.77,
         332.3078321524162
        ],
        [
         10800132,
         539194858,
         22.91,
         288.15195724562045
        ],
        [
         1005072,
         555447922,
         1044.09,
         355.84526557638407
        ],
        [
         1005143,
         517062545,
         1541.61,
         358.87128613985936
        ],
        [
         50600000,
         514933060,
         102.42,
         9.80243445393046
        ],
        [
         10800025,
         539194858,
         56.6,
         288.152710717548
        ],
        [
         1005101,
         539538500,
         450.46,
         357.0994503030662
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{\"ml_attr\": {}}",
         "name": "prediction",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "train_df, test_df = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Fit pipeline on training data\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = pipeline_model.transform(test_df)\n",
    "\n",
    "# Fix: call .limit(10) before display()\n",
    "display(predictions.select(\"product_id\", \"user_id\", \"price\", \"prediction\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d372d85-998f-41e7-9052-196bc27caefe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'r2_value': 0.054417169334161186}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate R2 value\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "r2_value = evaluator.evaluate(predictions)\n",
    "display({\"r2_value\": r2_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c13c387-3031-4543-a300-8f495fe2551f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  warnings.warn(\n2026/01/21 07:49:46 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/01/21 07:49:49 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-18f2f99c-09cf-4487-ac46-b6/tmpiw94j0l9/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \nRegistered model 'spark_linear_regression' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Linear Regression model logged with R2: 0.0544\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'workspace.default.spark_linear_regression'.\n"
     ]
    }
   ],
   "source": [
    "# Log the spark linear regression model in MLflow artifacts using mlflow.spark.log_model method and register the model in MLflow Model Registry using mlflow.register_model method \n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Infer signature from test data and predictions\n",
    "signature = infer_signature(test_df, predictions.select(\"prediction\"))\n",
    "\n",
    "with mlflow.start_run(run_name=\"spark_linear_regression\"):\n",
    "    mlflow.log_param(\"model_type\", \"spark_linear_regression\")\n",
    "    mlflow.log_param(\"features\", \"product_id, user_id\")\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=pipeline_model,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=\"spark_linear_regression\",\n",
    "        dfs_tmpdir=\"/Volumes/workspace/ecommerce/ecommerce_data\",  # Replace with your actual UC volume path\n",
    "        signature=signature\n",
    "    )\n",
    "    mlflow.log_metric(\"r2_score\", r2_value)\n",
    "    mlflow.set_tag(\"model_type\", \"spark_linear_regression\")\n",
    "    print(f\"Spark Linear Regression model logged with R2: {r2_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51169a24-6cf7-4f4d-be3a-d68d333b6f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets build spark decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a6b8cea-e892-40b9-aeba-c49f83219267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>user_id</th><th>price</th><th>prediction</th></tr></thead><tbody><tr><td>1005135</td><td>535871217</td><td>1747.79</td><td>1247.9223399252578</td></tr><tr><td>1005073</td><td>543427258</td><td>1207.71</td><td>356.80787772282997</td></tr><tr><td>10900026</td><td>514080443</td><td>40.8</td><td>133.9266503171796</td></tr><tr><td>4802639</td><td>514808401</td><td>218.77</td><td>164.61008266537212</td></tr><tr><td>10800132</td><td>539194858</td><td>22.91</td><td>133.9266503171796</td></tr><tr><td>1005072</td><td>555447922</td><td>1044.09</td><td>356.80787772282997</td></tr><tr><td>1005143</td><td>517062545</td><td>1541.61</td><td>1252.1573979085813</td></tr><tr><td>50600000</td><td>514933060</td><td>102.42</td><td>263.42773152243905</td></tr><tr><td>10800025</td><td>539194858</td><td>56.6</td><td>133.9266503171796</td></tr><tr><td>1005101</td><td>539538500</td><td>450.46</td><td>356.80787772282997</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1005135,
         535871217,
         1747.79,
         1247.9223399252578
        ],
        [
         1005073,
         543427258,
         1207.71,
         356.80787772282997
        ],
        [
         10900026,
         514080443,
         40.8,
         133.9266503171796
        ],
        [
         4802639,
         514808401,
         218.77,
         164.61008266537212
        ],
        [
         10800132,
         539194858,
         22.91,
         133.9266503171796
        ],
        [
         1005072,
         555447922,
         1044.09,
         356.80787772282997
        ],
        [
         1005143,
         517062545,
         1541.61,
         1252.1573979085813
        ],
        [
         50600000,
         514933060,
         102.42,
         263.42773152243905
        ],
        [
         10800025,
         539194858,
         56.6,
         133.9266503171796
        ],
        [
         1005101,
         539538500,
         450.46,
         356.80787772282997
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{\"ml_attr\": {}}",
         "name": "prediction",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'decision_tree_r2': 0.38181019200492583}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train decision tree model using spark\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"price\",\n",
    "    maxDepth=5\n",
    ")\n",
    "\n",
    "# Build pipeline with assembler and decision tree\n",
    "dt_pipeline = Pipeline(stages=[assembler, dt])\n",
    "\n",
    "# Fit pipeline on training data\n",
    "dt_model = dt_pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on test data\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "\n",
    "display(dt_predictions.select(\"product_id\", \"user_id\", \"price\", \"prediction\").limit(10))\n",
    "\n",
    "# Calculate R2 value\n",
    "dt_r2 = evaluator.evaluate(dt_predictions)\n",
    "display({\"decision_tree_r2\": dt_r2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66bde02f-227f-4297-8cf4-dd1d22c26670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  warnings.warn(\n2026/01/21 07:51:27 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/01/21 07:51:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-18f2f99c-09cf-4487-ac46-b6/tmp7chxtpco/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \nRegistered model 'spark_decision_tree_regressor' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Decision Tree Regressor model logged with R2: 0.3818\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'workspace.default.spark_decision_tree_regressor'.\n"
     ]
    }
   ],
   "source": [
    "# Log the spark decision tree regressor in MLflow artifacts using mlflow.spark.log_model method and register the model in MLflow Model Registry using mlflow.register_model method \n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Infer signature from test data and predictions\n",
    "dt_signature = infer_signature(test_df, dt_predictions.select(\"prediction\"))\n",
    "\n",
    "with mlflow.start_run(run_name=\"spark_decision_tree_regressor\"):\n",
    "    mlflow.log_param(\"model_type\", \"spark_decision_tree_regressor\")\n",
    "    mlflow.log_param(\"features\", \"product_id, user_id\")\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=dt_model,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=\"spark_decision_tree_regressor\",\n",
    "        dfs_tmpdir=\"/Volumes/workspace/ecommerce/ecommerce_data\",\n",
    "        signature=dt_signature\n",
    "    )\n",
    "    mlflow.log_metric(\"r2_score\", dt_r2)\n",
    "    mlflow.set_tag(\"model_type\", \"spark_decision_tree_regressor\")\n",
    "    print(f\"Spark Decision Tree Regressor model logged with R2: {dt_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48204b83-9edd-4b64-bcd8-08d693b7db4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task-4 Select best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b57ed82-40e3-49e7-892d-0c89ef8c7747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Why this is the correct choice:**\n",
    "\n",
    "Sklearn Decision tree model got Highest R² score\n",
    "\n",
    "\n",
    "**Spark Pipeline ensures:**\n",
    "\n",
    "Uses Spark-native execution\n",
    "\n",
    "Feature consistency\n",
    "\n",
    "Reproducibility\n",
    "\n",
    "Production readiness\n",
    "\n",
    "Easily schedulable as a Databricks Job\n",
    "\n",
    "Can scale when data grows (today thousands → tomorrow millions)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day13",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}